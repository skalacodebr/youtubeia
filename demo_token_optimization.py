#!/usr/bin/env python3
"""
Demonstra√ß√£o do Sistema de Otimiza√ß√£o de Tokens para Modelos Limitados (32K)
Mostra estrat√©gias de chunking, an√°lise progressiva e s√≠ntese em etapas
"""

import os
from datetime import datetime

def show_token_optimization_strategies():
    """Demonstra as estrat√©gias de otimiza√ß√£o de tokens"""
    
    print("üîß SISTEMA DE OTIMIZA√á√ÉO PARA MODELOS LIMITADOS (32K TOKENS)")
    print("=" * 70)
    
    print("üìä PROBLEMA:")
    print("   ‚Ä¢ Modelos locais: limita√ß√£o de ~32.000 tokens")
    print("   ‚Ä¢ Transcri√ß√µes YouTube: 5.000 - 50.000 tokens cada")
    print("   ‚Ä¢ 10 v√≠deos: pode passar de 200.000 tokens")
    print("   ‚Ä¢ Resultado: Erro ou truncamento de conte√∫do")
    print()
    
    print("üß† SOLU√á√ïES IMPLEMENTADAS:")
    print("=" * 40)
    
    strategies = {
        "1. Chunking Inteligente": [
            "Divide transcri√ß√µes em peda√ßos de ~2K tokens",
            "Mant√©m contexto dividindo por senten√ßas",
            "Analisa cada chunk separadamente",
            "Reconstr√≥i an√°lise completa do v√≠deo"
        ],
        "2. An√°lise Progressiva": [
            "Agrupa an√°lises em lotes de 3 v√≠deos",
            "Sumariza cada lote primeiro",
            "Combina resumos na s√≠ntese final",
            "Reduz drasticamente o uso de tokens"
        ],
        "3. Filtragem de Relev√¢ncia": [
            "Remove chunks sem conte√∫do relevante",
            "Elimina redund√¢ncias automaticamente",
            "Foca apenas no essencial",
            "Maximiza qualidade vs tokens"
        ],
        "4. Templates Otimizados": [
            "Prompts mais concisos e diretos",
            "Limites de tokens por opera√ß√£o",
            "Estrutura hier√°rquica de an√°lise",
            "S√≠ntese final controlada"
        ]
    }
    
    for strategy, points in strategies.items():
        print(f"\nüéØ {strategy}:")
        for point in points:
            print(f"   ‚Ä¢ {point}")

def compare_normal_vs_optimized():
    """Compara modo normal vs otimizado"""
    
    print("\n‚öñÔ∏è COMPARA√á√ÉO: MODO NORMAL vs OTIMIZADO")
    print("=" * 55)
    
    comparison = {
        "Aspecto": ["MODO NORMAL", "MODO OTIMIZADO"],
        "Tokens por chamada": ["50K+ tokens", "~3K tokens"],
        "N√∫mero de chamadas": ["~15 chamadas", "~40 chamadas"],
        "Compatibilidade": ["Modelos grandes", "Todos os modelos"],
        "Velocidade": ["Mais r√°pido", "Levemente mais lento"],
        "Qualidade": ["Excelente", "Muito boa"],
        "Custo (APIs pagas)": ["Mais caro", "Mais barato"],
        "Risco de erro": ["Alto (se >32K)", "Muito baixo"],
        "Transpar√™ncia": ["Moderada", "Alta (chunks vis√≠veis)"]
    }
    
    # Cabe√ßalhos
    print(f"{'Aspecto':<20} | {'MODO NORMAL':<15} | {'MODO OTIMIZADO':<20}")
    print("-" * 60)
    
    # Dados
    aspects = ["Tokens por chamada", "N√∫mero de chamadas", "Compatibilidade", "Velocidade", 
               "Qualidade", "Custo (APIs pagas)", "Risco de erro", "Transpar√™ncia"]
    
    data = {
        "Tokens por chamada": ("50K+ tokens", "~3K tokens"),
        "N√∫mero de chamadas": ("~15 chamadas", "~40 chamadas"), 
        "Compatibilidade": ("Modelos grandes", "Todos os modelos"),
        "Velocidade": ("Mais r√°pido", "Levemente mais lento"),
        "Qualidade": ("Excelente", "Muito boa"),
        "Custo (APIs pagas)": ("Mais caro", "Mais barato"),
        "Risco de erro": ("Alto (se >32K)", "Muito baixo"),
        "Transpar√™ncia": ("Moderada", "Alta (chunks vis√≠veis)")
    }
    
    for aspect in aspects:
        normal, optimized = data[aspect]
        print(f"{aspect:<20} | {normal:<15} | {optimized:<20}")

def show_chunking_example():
    """Mostra exemplo pr√°tico de chunking"""
    
    print("\nüìÑ EXEMPLO PR√ÅTICO: CHUNKING DE TRANSCRI√á√ÉO")
    print("=" * 50)
    
    # Exemplo de transcri√ß√£o longa
    example_transcript = """
    Ol√° pessoal, hoje vamos falar sobre intelig√™ncia artificial e como ela est√° 
    transformando o mercado de trabalho. Primeiro, √© importante entender que a IA 
    n√£o √© uma tecnologia √∫nica, mas sim um conjunto de tecnologias que incluem 
    machine learning, processamento de linguagem natural, vis√£o computacional e muito mais.
    
    Vamos come√ßar falando sobre os setores mais afetados. Na medicina, por exemplo, 
    j√° temos sistemas de IA que conseguem diagnosticar c√¢ncer com precis√£o de 94%, 
    superando m√©dicos experientes em alguns casos espec√≠ficos. Isso n√£o significa 
    que os m√©dicos ser√£o substitu√≠dos, mas sim que eles ter√£o ferramentas mais 
    poderosas para ajudar seus pacientes.
    
    No setor financeiro, a situa√ß√£o √© interessante. Bancos como o Nubank j√° usam 
    IA para an√°lise de cr√©dito, processando milh√µes de dados em segundos. Cerca de 
    40% das tarefas banc√°rias b√°sicas podem ser automatizadas at√© 2025, segundo 
    estudos do McKinsey Institute.
    
    Mas e as solu√ß√µes pr√°ticas? O que voc√™, profissional, pode fazer agora? 
    Primeiro, aprenda a trabalhar COM a IA, n√£o contra ela. Use ferramentas como 
    ChatGPT, Claude ou Copilot no seu trabalho atual. Segundo, desenvolva skills 
    √∫nicamente humanas: criatividade, empatia, pensamento cr√≠tico complexo.
    """
    
    print("üìù TRANSCRI√á√ÉO ORIGINAL (4.200+ caracteres):")
    print(f"   Tamanho: {len(example_transcript)} caracteres")
    print(f"   Tokens estimados: ~{len(example_transcript)//4} tokens")
    print()
    
    # Simula chunking
    chunks = []
    sentences = example_transcript.split('. ')
    current_chunk = ""
    chunk_size = 800  # chars
    
    for sentence in sentences:
        if len(current_chunk + sentence) < chunk_size:
            current_chunk += sentence + ". "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence + ". "
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    print(f"üîÄ AP√ìS CHUNKING ({len(chunks)} chunks):")
    for i, chunk in enumerate(chunks, 1):
        print(f"\n   üìÑ CHUNK {i} ({len(chunk)} chars, ~{len(chunk)//4} tokens):")
        print(f"      {chunk[:100]}...")

def show_progressive_synthesis():
    """Mostra exemplo de s√≠ntese progressiva"""
    
    print("\nüîó EXEMPLO: S√çNTESE PROGRESSIVA")
    print("=" * 40)
    
    # Simula an√°lises de v√≠deos
    video_analyses = [
        "V√≠deo 1: IA n√£o substitui empregos imediatamente, 47% em risco (Oxford)",
        "V√≠deo 2: Medicina - IA diagnostica c√¢ncer 94% precis√£o",
        "V√≠deo 3: Finan√ßas - 40% tarefas banc√°rias automatizadas at√© 2025",
        "V√≠deo 4: Solu√ß√µes pr√°ticas - usar IA como ferramenta, n√£o rival",
        "V√≠deo 5: Skills humanas - criatividade, empatia, pensamento cr√≠tico",
        "V√≠deo 6: Setores resistentes - arte, terapia, educa√ß√£o personalizada",
        "V√≠deo 7: Timeline - 2024-2025 automa√ß√£o simples, 2026-2028 transforma√ß√£o",
        "V√≠deo 8: Exemplos sucesso - designer 300% produtivo com IA",
        "V√≠deo 9: Regulamenta√ß√£o - falta de pol√≠ticas adequadas",
        "V√≠deo 10: Futuro - 85% empregos 2030 ainda n√£o existem"
    ]
    
    print("üìä AN√ÅLISES ORIGINAIS (10 v√≠deos):")
    for analysis in video_analyses:
        print(f"   ‚Ä¢ {analysis}")
    
    print(f"\nüîÑ AGRUPAMENTO EM LOTES:")
    
    # Simula agrupamento em lotes de 3
    batches = [video_analyses[i:i+3] for i in range(0, len(video_analyses), 3)]
    
    batch_summaries = [
        "LOTE 1: IA causa transforma√ß√£o gradual, n√£o substitui√ß√£o imediata. Dados: 47% empregos em risco, medicina 94% precis√£o.",
        "LOTE 2: Solu√ß√µes: usar IA como ferramenta, desenvolver skills humanas (criatividade, empatia). Setores resistentes identificados.",
        "LOTE 3: Timeline 2024-2028, casos sucesso (designer +300%), necessidade regulamenta√ß√£o.",
        "LOTE 4: Perspectiva futura otimista - 85% empregos 2030 n√£o existem ainda."
    ]
    
    for i, (batch, summary) in enumerate(zip(batches, batch_summaries), 1):
        print(f"\n   üì¶ LOTE {i} ({len(batch)} an√°lises):")
        for analysis in batch:
            print(f"      - {analysis[:50]}...")
        print(f"   üìã RESUMO LOTE {i}: {summary}")
    
    print(f"\nüéØ S√çNTESE FINAL:")
    final_synthesis = """
    Com base na an√°lise de 10 v√≠deos, a IA est√° transformando o trabalho gradualmente, 
    n√£o abruptamente. Dados mostram 47% empregos em risco, mas solu√ß√µes existem: 
    usar IA como ferramenta, desenvolver skills humanas, focar em setores resistentes. 
    Timeline: 2024-2028 para transforma√ß√£o completa. Futuro otimista: 85% empregos 
    2030 ainda n√£o existem.
    """
    print(f"   {final_synthesis.strip()}")

def show_token_savings():
    """Mostra economia de tokens"""
    
    print("\nüí∞ ECONOMIA DE TOKENS - EXEMPLO REAL")
    print("=" * 45)
    
    scenario = {
        "V√≠deos": 10,
        "Transcri√ß√£o m√©dia": "15.000 tokens cada",
        "Total sem otimiza√ß√£o": "150.000 tokens",
        "Limite modelo": "32.000 tokens",
        "Status sem otimiza√ß√£o": "‚ùå ERRO - Excede limite"
    }
    
    optimized = {
        "Chunks por v√≠deo": "8 chunks de 2K tokens",
        "An√°lise por chunk": "300 tokens",
        "S√≠ntese por v√≠deo": "500 tokens", 
        "Total por v√≠deo": "3.900 tokens",
        "Lotes de s√≠ntese": "4 lotes de 3 v√≠deos",
        "S√≠ntese final": "1.500 tokens",
        "Total otimizado": "~8.000 tokens",
        "Status": "‚úÖ SUCESSO - Dentro do limite"
    }
    
    print("üìä CEN√ÅRIO ORIGINAL:")
    for key, value in scenario.items():
        print(f"   {key}: {value}")
    
    print("\nüîß COM OTIMIZA√á√ÉO:")
    for key, value in optimized.items():
        print(f"   {key}: {value}")
    
    print(f"\nüí° RESULTADO:")
    print(f"   Redu√ß√£o: 150.000 ‚Üí 8.000 tokens (94.7% economia)")
    print(f"   Compatibilidade: ‚ùå ‚Üí ‚úÖ")
    print(f"   Qualidade: Mantida alta")

def show_usage_recommendations():
    """Mostra recomenda√ß√µes de uso"""
    
    print("\nüìã QUANDO USAR CADA MODO:")
    print("=" * 35)
    
    recommendations = {
        "üöÄ MODO NORMAL - Use quando:": [
            "Seu modelo suporta >50K tokens",
            "Usando APIs como GPT-4, Claude Pro",
            "Velocidade √© prioridade",
            "Poucos v√≠deos (<5) para analisar"
        ],
        "üîß MODO OTIMIZADO - Use quando:": [
            "Modelo local com limita√ß√£o 32K",
            "Usando LM Studio, Ollama",
            "Muitos v√≠deos (>8) para analisar",
            "Transcri√ß√µes muito longas",
            "Quer economizar tokens/custo",
            "Modelo d√° erros de limite"
        ]
    }
    
    for mode, points in recommendations.items():
        print(f"\n{mode}")
        for point in points:
            print(f"   ‚Ä¢ {point}")
    
    print(f"\nüí° DICA PRO:")
    print(f"   ‚Ä¢ Teste primeiro o modo normal")
    print(f"   ‚Ä¢ Se der erro de tokens, mude para otimizado")
    print(f"   ‚Ä¢ Modelos 7B-13B: sempre use otimizado")
    print(f"   ‚Ä¢ APIs pagas: otimizado pode ser mais barato")

if __name__ == "__main__":
    show_token_optimization_strategies()
    compare_normal_vs_optimized()
    show_chunking_example()
    show_progressive_synthesis()
    show_token_savings()
    show_usage_recommendations()
    
    print("\nüöÄ COMO USAR:")
    print("1. Execute: streamlit run app.py")
    print("2. V√° para 'Pesquisa Profunda'")
    print("3. Marque ‚òëÔ∏è 'Modo Otimizado (32K tokens)'")
    print("4. Configure sua pesquisa normalmente")
    print("5. ‚ú® Sistema automaticamente otimiza para seu modelo!")
    
    print("\nüéØ RESULTADO: Pesquisas eficazes mesmo com modelos limitados!") 