import streamlit as st
import sqlite3
import os
from datetime import datetime, timedelta
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
from openai import OpenAI
from typing import List, Dict, Optional
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re

# Configura√ß√µes
YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY") or "AIzaSyCxwP28gezkKP2uk9Kw6O6b9SlK7foRlqE"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")  # Configure sua chave OpenAI
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL")  # URL customizada para APIs compat√≠veis
OPENAI_MODEL = os.getenv("OPENAI_MODEL") or "gpt-3.5-turbo"  # Modelo customizado
DB_NAME = "transcripts.db"
MAX_RESULTS = 10

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="YouTube AI Chat",
    page_icon="üé•",
    layout="wide"
)

# Inicializar OpenAI
openai_client = None
if OPENAI_API_KEY or OPENAI_BASE_URL:
    # Configura√ß√£o para APIs OpenAI compat√≠veis
    client_config = {}
    
    if OPENAI_API_KEY:
        client_config["api_key"] = OPENAI_API_KEY
    else:
        # Para APIs locais que n√£o precisam de key, usa uma dummy
        client_config["api_key"] = "dummy-key"
    
    if OPENAI_BASE_URL:
        client_config["base_url"] = OPENAI_BASE_URL
    
    openai_client = OpenAI(**client_config)

def create_openai_client(api_key: str, base_url: Optional[str] = None) -> Optional[OpenAI]:
    """Cria cliente OpenAI de forma segura."""
    try:
        if base_url:
            return OpenAI(api_key=api_key, base_url=base_url)
        else:
            return OpenAI(api_key=api_key)
    except Exception as e:
        st.error(f"Erro ao criar cliente OpenAI: {e}")
        return None

class YouTubeRAGSystem:
    def __init__(self):
        self.create_database()
        
    def create_database(self):
        """Cria o banco de dados SQLite."""
        conn = sqlite3.connect(DB_NAME)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS transcripts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                video_id TEXT NOT NULL,
                title TEXT NOT NULL,
                topic TEXT NOT NULL,
                transcript TEXT,
                published_date TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        conn.commit()
        conn.close()

    def search_youtube_videos(self, topic: str) -> List[Dict]:
        """Busca v√≠deos no YouTube."""
        youtube = build("youtube", "v3", developerKey=YOUTUBE_API_KEY)
        
        three_months_ago = datetime.now() - timedelta(days=90)
        published_after = three_months_ago.strftime('%Y-%m-%dT%H:%M:%SZ')
        
        request = youtube.search().list(
            part="snippet",
            q=topic,
            type="video",
            maxResults=MAX_RESULTS,
            order="relevance",
            publishedAfter=published_after,
            regionCode="BR",
            relevanceLanguage="pt"
        )
        
        response = request.execute()
        videos = []
        
        for item in response["items"]:
            video_id = item["id"]["videoId"]
            title = item["snippet"]["title"]
            published_date = item["snippet"]["publishedAt"]
            videos.append({
                "video_id": video_id,
                "title": title,
                "published_date": published_date
            })
        
        return videos

    def get_transcript(self, video_id: str) -> Optional[str]:
        """Obt√©m transcri√ß√£o de um v√≠deo."""
        try:
            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=["pt", "en"])
            return " ".join([entry["text"] for entry in transcript])
        except Exception as e:
            return None

    def process_videos(self, topic: str) -> int:
        """Processa v√≠deos e salva transcri√ß√µes."""
        videos = self.search_youtube_videos(topic)
        saved_count = 0
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, video in enumerate(videos):
            video_id = video["video_id"]
            title = video["title"]
            published_date = video["published_date"]
            
            status_text.text(f"Processando: {title[:50]}...")
            
            # Verifica se j√° existe no banco
            if not self.video_exists(video_id):
                transcript = self.get_transcript(video_id)
                
                if transcript:
                    self.save_transcript(video_id, title, topic, transcript, published_date)
                    saved_count += 1
            
            progress_bar.progress((i + 1) / len(videos))
        
        status_text.text(f"Processamento conclu√≠do! {saved_count} novas transcri√ß√µes salvas.")
        return saved_count

    def video_exists(self, video_id: str) -> bool:
        """Verifica se o v√≠deo j√° existe no banco."""
        conn = sqlite3.connect(DB_NAME)
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM transcripts WHERE video_id = ?", (video_id,))
        exists = cursor.fetchone()[0] > 0
        conn.close()
        return exists

    def save_transcript(self, video_id: str, title: str, topic: str, transcript: str, published_date: str):
        """Salva transcri√ß√£o no banco."""
        conn = sqlite3.connect(DB_NAME)
        cursor = conn.cursor()
        cursor.execute("""
            INSERT INTO transcripts (video_id, title, topic, transcript, published_date)
            VALUES (?, ?, ?, ?, ?)
        """, (video_id, title, topic, transcript, published_date))
        conn.commit()
        conn.close()

    def get_transcripts_by_topic(self, topic: str) -> List[Dict]:
        """Obt√©m transcri√ß√µes por t√≥pico."""
        conn = sqlite3.connect(DB_NAME)
        cursor = conn.cursor()
        cursor.execute("""
            SELECT video_id, title, transcript, published_date 
            FROM transcripts 
            WHERE topic LIKE ? AND transcript IS NOT NULL
        """, (f"%{topic}%",))
        
        results = cursor.fetchall()
        conn.close()
        
        return [
            {
                "video_id": row[0],
                "title": row[1],
                "transcript": row[2],
                "published_date": row[3]
            }
            for row in results
        ]

    def find_relevant_context(self, query: str, topic: str, max_context: int = 3) -> str:
        """Encontra contexto relevante usando TF-IDF."""
        transcripts = self.get_transcripts_by_topic(topic)
        
        if not transcripts:
            return "Nenhuma transcri√ß√£o encontrada para este t√≥pico."
        
        # Prepara textos para vectoriza√ß√£o
        documents = [t["transcript"] for t in transcripts]
        titles = [t["title"] for t in transcripts]
        
        # Se h√° poucos documentos, retorna todos
        if len(documents) <= max_context:
            context_parts = []
            for i, (title, doc) in enumerate(zip(titles, documents)):
                context_parts.append(f"**{title}**\n{doc[:800]}...")
            return "\n\n".join(context_parts)
        
        try:
            # Adiciona a query aos documentos
            all_docs = documents + [query.lower()]
            
            # Vectoriza√ß√£o TF-IDF com configura√ß√µes para portugu√™s
            vectorizer = TfidfVectorizer(
                max_features=1000,
                stop_words=None,  # Remove stop_words espec√≠ficas
                min_df=1,
                ngram_range=(1, 2)  # Inclui bigramas
            )
            tfidf_matrix = vectorizer.fit_transform(all_docs)
            
            # Calcula similaridade entre query e documentos
            query_vec = tfidf_matrix[-1]  # √öltimo √© a query
            doc_vecs = tfidf_matrix[:-1]  # Todos exceto a query
            
            similarities = cosine_similarity(query_vec, doc_vecs).flatten()
            
            # Ordena por similaridade
            top_indices = similarities.argsort()[-max_context:][::-1]
            
            # Constr√≥i contexto com threshold mais baixo
            context_parts = []
            for idx in top_indices:
                if similarities[idx] > 0.01:  # Threshold muito menor
                    context_parts.append(f"**{titles[idx]}**\n{documents[idx][:800]}...")
            
            # Se ainda n√£o encontrou nada, pega os melhores mesmo assim
            if not context_parts:
                for idx in top_indices[:max_context]:
                    context_parts.append(f"**{titles[idx]}**\n{documents[idx][:800]}...")
                    
            return "\n\n".join(context_parts) if context_parts else "Erro na busca de contexto."
            
        except Exception as e:
            # Fallback: retorna os primeiros documentos
            context_parts = []
            for i, (title, doc) in enumerate(zip(titles[:max_context], documents[:max_context])):
                context_parts.append(f"**{title}**\n{doc[:800]}...")
            return "\n\n".join(context_parts)

    def chat_with_openai(self, query: str, context: str) -> str:
        """Chat com OpenAI usando contexto RAG."""
        # Usa o cliente do session_state se dispon√≠vel, sen√£o usa o global
        client = getattr(st.session_state, 'openai_client', None) or openai_client
        model = getattr(st.session_state, 'current_model', None) or OPENAI_MODEL
        
        if not client:
            return "Por favor, configure sua API OpenAI ou compat√≠vel."
        
        prompt = f"""
        Voc√™ √© um assistente especializado em responder perguntas baseadas em transcri√ß√µes de v√≠deos do YouTube.
        
        CONTEXTO (das transcri√ß√µes dos v√≠deos):
        {context}
        
        PERGUNTA DO USU√ÅRIO: {query}
        
        Responda de forma √∫til e informativa, baseando-se principalmente no contexto fornecido. 
        Se a informa√ß√£o n√£o estiver no contexto, mencione isso claramente.
        """
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "Voc√™ √© um assistente especializado em analisar conte√∫do de v√≠deos do YouTube."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            
            return response.choices[0].message.content or "Desculpe, n√£o consegui gerar uma resposta."
        except Exception as e:
            return f"Erro ao comunicar com a API: {str(e)}"

class DeepResearchSystem:
    def __init__(self, rag_system):
        self.rag_system = rag_system
    
    def analyze_individual_video(self, video_data: Dict, research_query: str) -> Dict:
        """Analisa um v√≠deo individual para extrair insights espec√≠ficos."""
        client = getattr(st.session_state, 'openai_client', None) or openai_client
        model = getattr(st.session_state, 'current_model', None) or OPENAI_MODEL
        
        if not client:
            return {"error": "Cliente OpenAI n√£o configurado"}
        
        # Pega uma amostra do transcript para an√°lise
        transcript_sample = video_data["transcript"][:3000]  # Primeiros 3000 chars
        
        analysis_prompt = f"""
        Voc√™ √© um pesquisador especializado em an√°lise de conte√∫do de v√≠deos.
        
        V√çDEO ANALISADO: {video_data["title"]}
        
        TRANSCRI√á√ÉO (AMOSTRA):
        {transcript_sample}
        
        PERGUNTA DE PESQUISA: {research_query}
        
        Por favor, analise este v√≠deo e extraia:
        1. **INSIGHTS PRINCIPAIS**: 3-5 pontos principais relacionados √† pergunta
        2. **DADOS/ESTAT√çSTICAS**: N√∫meros, fatos concretos mencionados
        3. **OPINI√ïES/PERSPECTIVAS**: Pontos de vista √∫nicos do autor
        4. **EXEMPLOS PR√ÅTICOS**: Casos ou exemplos espec√≠ficos
        5. **RELEV√ÇNCIA**: Como este v√≠deo contribui para responder a pergunta (1-10)
        
        Seja espec√≠fico e objetivo. Foque apenas no que √© relevante para a pergunta de pesquisa.
        """
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "Voc√™ √© um analista de conte√∫do especializado em extrair insights de v√≠deos do YouTube."},
                    {"role": "user", "content": analysis_prompt}
                ],
                max_tokens=800,
                temperature=0.3  # Mais determin√≠stico para an√°lise
            )
            
            return {
                "video_id": video_data["video_id"],
                "title": video_data["title"],
                "analysis": response.choices[0].message.content,
                "status": "success"
            }
            
        except Exception as e:
            return {
                "video_id": video_data["video_id"],
                "title": video_data["title"],
                "error": str(e),
                "status": "error"
            }
    
    def synthesize_insights(self, analyses: List[Dict], research_query: str, output_type: str) -> str:
        """Sintetiza todos os insights em um resultado final."""
        client = getattr(st.session_state, 'openai_client', None) or openai_client
        model = getattr(st.session_state, 'current_model', None) or OPENAI_MODEL
        
        if not client:
            return "Erro: Cliente OpenAI n√£o configurado"
        
        # Prepara o contexto com todas as an√°lises
        context = "## AN√ÅLISES INDIVIDUAIS DOS V√çDEOS:\n\n"
        for i, analysis in enumerate(analyses, 1):
            if analysis["status"] == "success":
                context += f"### V√çDEO {i}: {analysis['title']}\n"
                context += f"{analysis['analysis']}\n\n"
        
        # Templates para diferentes tipos de sa√≠da
        templates = {
            "script": """
            Crie um SCRIPT DE V√çDEO profissional baseado nas an√°lises. O script deve:
            - Ter introdu√ß√£o cativante
            - Desenvolver os pontos principais de forma l√≥gica
            - Incluir dados e exemplos das an√°lises
            - Ter transi√ß√µes naturais entre t√≥picos
            - Terminar com conclus√£o impactante
            - Dura√ß√£o estimada: 5-8 minutos
            
            Formato: [INTRODU√á√ÉO] [DESENVOLVIMENTO] [CONCLUS√ÉO]
            """,
            
            "resumo": """
            Crie um RESUMO EXECUTIVO abrangente que:
            - Sintetize os principais insights
            - Destaque consensus e diverg√™ncias
            - Inclua dados e estat√≠sticas relevantes
            - Apresente conclus√µes claras
            - Sugira pr√≥ximos passos ou recomenda√ß√µes
            
            Formato: Texto corrido, bem estruturado e profissional
            """,
            
            "analise": """
            Fa√ßa uma AN√ÅLISE PROFUNDA que:
            - Compare diferentes perspectivas dos v√≠deos
            - Identifique padr√µes e tend√™ncias
            - Analise lacunas ou contradi√ß√µes
            - Avalie a qualidade das fontes
            - Forne√ßa insights √∫nicos baseados na s√≠ntese
            
            Formato: An√°lise acad√™mica estruturada
            """,
            
            "artigo": """
            Escreva um ARTIGO COMPLETO que:
            - Tenha t√≠tulo atrativo
            - Introduza o tema de forma envolvente
            - Desenvolva argumentos com base nas an√°lises
            - Use dados e exemplos dos v√≠deos
            - Inclua subt√≥picos bem organizados
            - Termine com reflex√µes finais
            
            Formato: Artigo de blog profissional
            """
        }
        
        synthesis_prompt = f"""
        Voc√™ √© um pesquisador s√™nior especializado em s√≠ntese de informa√ß√µes.
        
        PERGUNTA DE PESQUISA: {research_query}
        
        TIPO DE SA√çDA SOLICITADA: {output_type.upper()}
        
        CONTEXTO COM AN√ÅLISES:
        {context}
        
        INSTRU√á√ÉO ESPEC√çFICA:
        {templates.get(output_type, templates["resumo"])}
        
        Com base nas an√°lises individuais dos v√≠deos, crie um {output_type} abrangente que responda √† pergunta de pesquisa de forma completa e profissional. Use os insights, dados e exemplos extra√≠dos das an√°lises para fundamentar sua resposta.
        """
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": f"Voc√™ √© um especialista em criar {output_type}s profissionais baseados em pesquisa de m√∫ltiplas fontes."},
                    {"role": "user", "content": synthesis_prompt}
                ],
                max_tokens=2000,
                temperature=0.4
            )
            
            return response.choices[0].message.content or f"Erro ao gerar {output_type}"
            
        except Exception as e:
            return f"Erro na s√≠ntese: {str(e)}"
    
    def identify_research_gaps(self, analyses: List[Dict], research_query: str) -> List[str]:
        """Identifica pontos que precisam de mais aprofundamento."""
        client = getattr(st.session_state, 'openai_client', None) or openai_client
        model = getattr(st.session_state, 'current_model', None) or OPENAI_MODEL
        
        if not client:
            return []
        
        # Prepara contexto das an√°lises
        context = "## AN√ÅLISES REALIZADAS:\n\n"
        for i, analysis in enumerate(analyses, 1):
            if analysis["status"] == "success":
                context += f"### V√çDEO {i}: {analysis['title']}\n"
                context += f"{analysis['analysis'][:500]}...\n\n"
        
        gap_analysis_prompt = f"""
        Voc√™ √© um pesquisador especializado em identificar lacunas de conhecimento.
        
        PERGUNTA DE PESQUISA: {research_query}
        
        AN√ÅLISES ATUAIS:
        {context}
        
        Com base nas an√°lises realizadas, identifique 3-5 PONTOS ESPEC√çFICOS que precisam de mais aprofundamento para responder completamente √† pergunta de pesquisa.
        
        Para cada ponto, forne√ßa:
        1. **T√≥pico espec√≠fico** (m√°ximo 6 palavras)
        2. **Por que precisa ser aprofundado** (1 frase)
        
        Formato de resposta:
        PONTO 1: [t√≥pico espec√≠fico]
        RAZ√ÉO: [por que precisa ser aprofundado]
        
        PONTO 2: [t√≥pico espec√≠fico]
        RAZ√ÉO: [por que precisa ser aprofundado]
        
        Seja espec√≠fico e focado em aspectos que realmente faltam na an√°lise atual.
        """
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "Voc√™ √© um especialista em identificar lacunas de pesquisa e pontos que precisam de aprofundamento."},
                    {"role": "user", "content": gap_analysis_prompt}
                ],
                max_tokens=600,
                temperature=0.3
            )
            
            response_text = response.choices[0].message.content or ""
            
            # Extrai pontos espec√≠ficos da resposta
            gaps = []
            lines = response_text.split('\n')
            current_point = None
            current_reason = None
            
            for line in lines:
                line = line.strip()
                if line.startswith("PONTO"):
                    if current_point and current_reason:
                        gaps.append(f"{current_point} ({current_reason})")
                    # Extrai o t√≥pico ap√≥s os dois pontos
                    current_point = line.split(":", 1)[1].strip() if ":" in line else line
                    current_reason = None
                elif line.startswith("RAZ√ÉO"):
                    current_reason = line.split(":", 1)[1].strip() if ":" in line else line
            
            # Adiciona o √∫ltimo ponto se existir
            if current_point and current_reason:
                gaps.append(f"{current_point} ({current_reason})")
            
            return gaps[:5]  # M√°ximo 5 pontos
            
        except Exception as e:
            st.warning(f"Erro ao identificar lacunas: {e}")
            return []

    def search_focused_videos(self, focus_topic: str, original_topic: str) -> List[Dict]:
        """Busca v√≠deos focados em um t√≥pico espec√≠fico."""
        youtube = build("youtube", "v3", developerKey=YOUTUBE_API_KEY)
        
        # Combina o t√≥pico original com o foco espec√≠fico
        search_query = f"{original_topic} {focus_topic}"
        
        three_months_ago = datetime.now() - timedelta(days=90)
        published_after = three_months_ago.strftime('%Y-%m-%dT%H:%M:%SZ')
        
        try:
            request = youtube.search().list(
                part="snippet",
                q=search_query,
                type="video",
                maxResults=5,  # Menos v√≠deos, mais espec√≠ficos
                order="relevance",
                publishedAfter=published_after,
                regionCode="BR",
                relevanceLanguage="pt"
            )
            
            response = request.execute()
            videos = []
            
            for item in response["items"]:
                video_id = item["id"]["videoId"]
                title = item["snippet"]["title"]
                published_date = item["snippet"]["publishedAt"]
                
                # Verifica se j√° temos este v√≠deo no banco
                if not self.rag_system.video_exists(video_id):
                    transcript = self.rag_system.get_transcript(video_id)
                    if transcript:
                        # Salva no banco
                        self.rag_system.save_transcript(video_id, title, search_query, transcript, published_date)
                        videos.append({
                            "video_id": video_id,
                            "title": title,
                            "transcript": transcript,
                            "published_date": published_date
                        })
            
            return videos
            
        except Exception as e:
            st.error(f"Erro ao buscar v√≠deos focados: {e}")
            return []

    def conduct_deep_research(self, topic: str, research_query: str, output_type: str) -> Dict:
        """Conduz uma pesquisa profunda completa."""
        
        # Passo 1: Obter v√≠deos relacionados
        st.write("### üîç Passo 1: Buscando v√≠deos relacionados...")
        transcripts = self.rag_system.get_transcripts_by_topic(topic)
        
        if not transcripts:
            return {
                "status": "error",
                "message": f"Nenhum v√≠deo encontrado para o t√≥pico '{topic}'. Fa√ßa uma busca primeiro."
            }
        
        st.write(f"‚úÖ Encontrados {len(transcripts)} v√≠deos para an√°lise")
        
        # Passo 2: An√°lise individual de cada v√≠deo
        st.write("### üß† Passo 2: Analisando cada v√≠deo individualmente...")
        
        analyses = []
        progress_bar = st.progress(0)
        
        for i, video in enumerate(transcripts):
            st.write(f"üé• Analisando: {video['title'][:60]}...")
            
            analysis = self.analyze_individual_video(video, research_query)
            analyses.append(analysis)
            
            # Mostra o progresso
            progress_bar.progress((i + 1) / len(transcripts))
            
            # Mostra resultado da an√°lise individual
            if analysis["status"] == "success":
                with st.expander(f"üìä An√°lise: {video['title'][:50]}..."):
                    st.write(analysis["analysis"])
            else:
                st.warning(f"‚ö†Ô∏è Erro na an√°lise: {analysis.get('error', 'Erro desconhecido')}")
        
        successful_analyses = [a for a in analyses if a["status"] == "success"]
        
        if not successful_analyses:
            return {
                "status": "error",
                "message": "Nenhuma an√°lise foi bem-sucedida. Verifique a configura√ß√£o da API."
            }
        
        # Passo 3: Identificar lacunas de pesquisa
        st.write("### üî¨ Passo 3: Identificando pontos para aprofundamento...")
        
        research_gaps = self.identify_research_gaps(successful_analyses, research_query)
        
        if research_gaps:
            st.write("üéØ **Pontos identificados que podem ser aprofundados:**")
            for i, gap in enumerate(research_gaps, 1):
                st.write(f"   {i}. {gap}")
            
            # Interface para sele√ß√£o de pontos para aprofundar
            st.write("### ü§î Deseja aprofundar algum destes pontos?")
            
            selected_gaps = []
            cols = st.columns(min(len(research_gaps), 3))
            
            for i, gap in enumerate(research_gaps):
                with cols[i % 3]:
                    gap_topic = gap.split(" (")[0]  # Remove a raz√£o para mostrar s√≥ o t√≥pico
                    if st.button(f"üîç Aprofundar: {gap_topic}", key=f"gap_{i}"):
                        selected_gaps.append(gap_topic)
            
            # Se usu√°rio selecionou pontos para aprofundar
            if selected_gaps:
                st.write("### üìö Passo 4: Buscando conte√∫do adicional...")
                
                additional_analyses = []
                for gap_topic in selected_gaps:
                    st.write(f"üéØ Buscando v√≠deos sobre: **{gap_topic}**")
                    
                    focused_videos = self.search_focused_videos(gap_topic, topic)
                    
                    if focused_videos:
                        st.write(f"   ‚úÖ Encontrados {len(focused_videos)} v√≠deos espec√≠ficos")
                        
                        for video in focused_videos:
                            analysis = self.analyze_individual_video(video, f"{research_query} - foco em {gap_topic}")
                            if analysis["status"] == "success":
                                additional_analyses.append(analysis)
                                st.write(f"   üìä Analisado: {video['title'][:50]}...")
                    else:
                        st.write(f"   ‚ö†Ô∏è Nenhum v√≠deo adicional encontrado para {gap_topic}")
                
                # Combina an√°lises originais com as adicionais
                if additional_analyses:
                    st.write(f"‚úÖ Adicionadas {len(additional_analyses)} an√°lises espec√≠ficas")
                    successful_analyses.extend(additional_analyses)
                
                st.write("### üîó Passo 5: S√≠ntese final expandida...")
            else:
                st.write("### üîó Passo 4: S√≠ntese final...")
        else:
            st.write("### üîó Passo 4: S√≠ntese final...")
        
        st.write(f"‚úÖ Sintetizando insights de {len(successful_analyses)} an√°lises...")
        
        final_result = self.synthesize_insights(successful_analyses, research_query, output_type)
        
        return {
            "status": "success",
            "analyses_count": len(successful_analyses),
            "total_videos": len(transcripts),
            "final_result": final_result,
            "individual_analyses": successful_analyses,
            "research_gaps": research_gaps if research_gaps else []
        }

class TokenOptimizedResearch:
    def __init__(self, rag_system):
        self.rag_system = rag_system
        self.max_tokens = 28000  # Margem de seguran√ßa para 32K
        self.chunk_size = 2000   # Tamanho de cada chunk de transcri√ß√£o
        
    def estimate_tokens(self, text: str) -> int:
        """Estima n√∫mero de tokens (aproximadamente 4 chars = 1 token)"""
        return len(text) // 4
    
    def chunk_transcript(self, transcript: str, chunk_size: Optional[int] = None) -> List[str]:
        """Divide transcri√ß√£o em chunks menores"""
        if chunk_size is None:
            chunk_size = self.chunk_size
            
        # Divide por senten√ßas para manter contexto
        sentences = transcript.split('. ')
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            # Se adicionar esta senten√ßa n√£o ultrapassar o limite
            if len(current_chunk + sentence) < chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        # Adiciona o √∫ltimo chunk
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        return chunks
    
    def analyze_video_chunk(self, chunk: str, video_title: str, research_query: str, chunk_index: int) -> str:
        """Analisa um chunk espec√≠fico de um v√≠deo"""
        client = getattr(st.session_state, 'openai_client', None) or openai_client
        model = getattr(st.session_state, 'current_model', None) or OPENAI_MODEL
        
        if not client:
            return "Erro: Cliente n√£o configurado"
        
        analysis_prompt = f"""
        Analise este FRAGMENTO do v√≠deo "{video_title}" (parte {chunk_index + 1}):
        
        FRAGMENTO:
        {chunk}
        
        PERGUNTA: {research_query}
        
        Extraia APENAS deste fragmento:
        1. **INSIGHTS RELEVANTES** (2-3 pontos m√°ximo)
        2. **DADOS/N√öMEROS** (se houver)
        3. **EXEMPLOS PR√ÅTICOS** (se houver)
        
        Seja CONCISO. Foque apenas no que √© diretamente relevante √† pergunta.
        Se este fragmento n√£o cont√©m informa√ß√µes relevantes, responda apenas "FRAGMENTO SEM CONTE√öDO RELEVANTE".
        """
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "Voc√™ √© um analista especializado em extrair insights relevantes de fragmentos de texto."},
                    {"role": "user", "content": analysis_prompt}
                ],
                max_tokens=300,  # An√°lise concisa
                temperature=0.2
            )
            
            return response.choices[0].message.content or "Erro na an√°lise"
            
        except Exception as e:
            return f"Erro: {str(e)}"
    
    def synthesize_video_analysis(self, chunk_analyses: List[str], video_title: str, research_query: str) -> str:
        """Sintetiza an√°lises de chunks em uma an√°lise consolidada do v√≠deo"""
        client = getattr(st.session_state, 'openai_client', None) or openai_client
        model = getattr(st.session_state, 'current_model', None) or OPENAI_MODEL
        
        if not client:
            return "Erro: Cliente n√£o configurado"
        
        # Remove fragmentos irrelevantes
        relevant_analyses = [analysis for analysis in chunk_analyses 
                           if "FRAGMENTO SEM CONTE√öDO RELEVANTE" not in analysis]
        
        if not relevant_analyses:
            return f"V√çDEO: {video_title}\nNENHUM CONTE√öDO RELEVANTE ENCONTRADO"
        
        combined_analysis = "\n\n".join(relevant_analyses)
        
        synthesis_prompt = f"""
        Consolide estas an√°lises de fragmentos do v√≠deo "{video_title}":
        
        AN√ÅLISES DOS FRAGMENTOS:
        {combined_analysis}
        
        PERGUNTA: {research_query}
        
        Crie uma S√çNTESE CONSOLIDADA que contenha:
        1. **INSIGHTS PRINCIPAIS** (3-5 pontos)
        2. **DADOS/ESTAT√çSTICAS** (consolidados)
        3. **EXEMPLOS/CASOS** (√∫nicos)
        4. **RELEV√ÇNCIA** (1-10)
        
        Elimine redund√¢ncias e mantenha apenas o essencial.
        """
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "Voc√™ √© um especialista em consolidar an√°lises fragmentadas."},
                    {"role": "user", "content": synthesis_prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )
            
            return response.choices[0].message.content or "Erro na s√≠ntese"
            
        except Exception as e:
            return f"Erro: {str(e)}"
    
    def analyze_video_optimized(self, video_data: Dict, research_query: str) -> Dict:
        """Analisa um v√≠deo de forma otimizada para tokens limitados"""
        
        # Se a transcri√ß√£o √© pequena, analisa diretamente
        if self.estimate_tokens(video_data["transcript"]) < 2000:
            # Usa an√°lise normal para v√≠deos pequenos
            deep_research = DeepResearchSystem(self.rag_system)
            return deep_research.analyze_individual_video(video_data, research_query)
        
        # Para v√≠deos grandes, usa chunking
        st.write(f"   üìÑ V√≠deo longo detectado: {video_data['title'][:50]}...")
        st.write(f"   üîÄ Dividindo em chunks para an√°lise otimizada...")
        
        chunks = self.chunk_transcript(video_data["transcript"])
        chunk_analyses = []
        
        # Analisa cada chunk
        for i, chunk in enumerate(chunks):
            analysis = self.analyze_video_chunk(chunk, video_data["title"], research_query, i)
            chunk_analyses.append(analysis)
        
        # Sintetiza an√°lises dos chunks
        consolidated_analysis = self.synthesize_video_analysis(chunk_analyses, video_data["title"], research_query)
        
        return {
            "video_id": video_data["video_id"],
            "title": video_data["title"],
            "analysis": consolidated_analysis,
            "status": "success",
            "chunks_processed": len(chunks)
        }
    
    def progressive_synthesis(self, analyses: List[str], research_query: str, output_type: str) -> str:
        """S√≠ntese progressiva em m√∫ltiplas etapas para economizar tokens"""
        client = getattr(st.session_state, 'openai_client', None) or openai_client
        model = getattr(st.session_state, 'current_model', None) or OPENAI_MODEL
        
        if not client:
            return "Erro: Cliente n√£o configurado"
        
        # Passo 1: Agrupa an√°lises em lotes pequenos
        batch_size = 3  # 3 an√°lises por vez
        batches = [analyses[i:i + batch_size] for i in range(0, len(analyses), batch_size)]
        
        batch_summaries = []
        
        st.write(f"   üîÑ Processando {len(batches)} lotes de an√°lises...")
        
        # Passo 2: Sumariza cada lote
        for i, batch in enumerate(batches):
            st.write(f"   üìä Processando lote {i + 1}/{len(batches)}...")
            
            batch_content = "\n\n".join(batch)
            
            batch_prompt = f"""
            Sumarize estas an√°lises de v√≠deos relacionadas √† pergunta: "{research_query}"
            
            AN√ÅLISES:
            {batch_content}
            
            Crie um RESUMO CONSOLIDADO que contenha:
            1. **INSIGHTS PRINCIPAIS** (m√°ximo 5)
            2. **DADOS IMPORTANTES** (n√∫meros, estat√≠sticas)
            3. **EXEMPLOS RELEVANTES** (casos pr√°ticos)
            4. **PADR√ïES IDENTIFICADOS** (tend√™ncias)
            
            Seja conciso mas completo. Elimine redund√¢ncias.
            """
            
            try:
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "Voc√™ √© um especialista em consolidar m√∫ltiplas an√°lises."},
                        {"role": "user", "content": batch_prompt}
                    ],
                    max_tokens=600,
                    temperature=0.3
                )
                
                batch_summary = response.choices[0].message.content or f"Erro no lote {i + 1}"
                batch_summaries.append(batch_summary)
                
            except Exception as e:
                batch_summaries.append(f"Erro no lote {i + 1}: {str(e)}")
        
        # Passo 3: S√≠ntese final dos resumos dos lotes
        st.write("   üéØ Criando s√≠ntese final...")
        
        final_content = "\n\n".join(batch_summaries)
        
        # Templates otimizados para tokens limitados
        templates = {
            "script": "Crie um SCRIPT DE V√çDEO (5-7 min) com [INTRO][DESENVOLVIMENTO][CONCLUS√ÉO]. Use dados e exemplos dos resumos.",
            "resumo": "Crie um RESUMO EXECUTIVO estruturado com insights principais, dados e recomenda√ß√µes.",
            "analise": "Fa√ßa uma AN√ÅLISE COMPARATIVA identificando padr√µes, tend√™ncias e conclus√µes.",
            "artigo": "Escreva um ARTIGO com t√≠tulo, introdu√ß√£o, desenvolvimento por t√≥picos e conclus√£o."
        }
        
        final_prompt = f"""
        PERGUNTA DE PESQUISA: {research_query}
        
        RESUMOS CONSOLIDADOS DOS V√çDEOS:
        {final_content}
        
        TAREFA: {templates.get(output_type, templates["resumo"])}
        
        Use todos os insights, dados e exemplos dos resumos para criar um {output_type} profissional e completo.
        Mantenha estrutura clara e linguagem envolvente.
        """
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": f"Voc√™ √© um especialista em criar {output_type}s profissionais baseados em pesquisa."},
                    {"role": "user", "content": final_prompt}
                ],
                max_tokens=1500,
                temperature=0.4
            )
            
            return response.choices[0].message.content or f"Erro ao gerar {output_type}"
            
        except Exception as e:
            return f"Erro na s√≠ntese final: {str(e)}"
    
    def conduct_optimized_research(self, topic: str, research_query: str, output_type: str) -> Dict:
        """Conduz pesquisa profunda otimizada para modelos com limita√ß√£o de tokens"""
        
        st.write("### üîç Passo 1: Buscando v√≠deos relacionados...")
        transcripts = self.rag_system.get_transcripts_by_topic(topic)
        
        if not transcripts:
            return {
                "status": "error",
                "message": f"Nenhum v√≠deo encontrado para o t√≥pico '{topic}'. Fa√ßa uma busca primeiro."
            }
        
        st.write(f"‚úÖ Encontrados {len(transcripts)} v√≠deos para an√°lise")
        
        # Estimativa de tokens totais
        total_chars = sum(len(video["transcript"]) for video in transcripts)
        estimated_tokens = total_chars // 4
        st.write(f"üìä Estimativa de tokens: {estimated_tokens:,}")
        
        if estimated_tokens > self.max_tokens:
            st.warning(f"‚ö†Ô∏è Conte√∫do extenso detectado. Usando an√°lise otimizada por chunks.")
        
        # Passo 2: An√°lise otimizada de cada v√≠deo
        st.write("### üß† Passo 2: An√°lise otimizada por v√≠deo...")
        
        analyses = []
        progress_bar = st.progress(0)
        
        for i, video in enumerate(transcripts):
            st.write(f"üé• Analisando: {video['title'][:60]}...")
            
            analysis = self.analyze_video_optimized(video, research_query)
            
            if analysis["status"] == "success":
                analyses.append(analysis["analysis"])
                
                # Mostra resultado com informa√ß√£o de chunks se aplic√°vel
                title_display = video['title'][:50] + "..."
                if "chunks_processed" in analysis:
                    title_display += f" ({analysis['chunks_processed']} chunks)"
                
                with st.expander(f"üìä An√°lise: {title_display}"):
                    st.write(analysis["analysis"])
            else:
                st.warning(f"‚ö†Ô∏è Erro na an√°lise: {analysis.get('error', 'Erro desconhecido')}")
            
            progress_bar.progress((i + 1) / len(transcripts))
        
        if not analyses:
            return {
                "status": "error",
                "message": "Nenhuma an√°lise foi bem-sucedida. Verifique a configura√ß√£o da API."
            }
        
        # Passo 3: S√≠ntese progressiva
        st.write("### üîó Passo 3: S√≠ntese progressiva otimizada...")
        
        final_result = self.progressive_synthesis(analyses, research_query, output_type)
        
        return {
            "status": "success",
            "analyses_count": len(analyses),
            "total_videos": len(transcripts),
            "final_result": final_result,
            "token_optimized": True,
            "estimated_tokens_saved": max(0, estimated_tokens - self.max_tokens)
        }

# Interface Streamlit
def main():
    st.title("üé• YouTube AI Chat com RAG")
    st.markdown("Busque v√≠deos do YouTube e converse sobre o conte√∫do usando IA!")
    
    rag_system = YouTubeRAGSystem()
    
    # Inicializar configura√ß√µes no session_state
    if "api_configured" not in st.session_state:
        st.session_state.api_configured = bool(OPENAI_API_KEY or OPENAI_BASE_URL)
        st.session_state.api_type = "OpenAI Official" if OPENAI_API_KEY else "OpenAI Compatible"
        st.session_state.openai_client = openai_client
        st.session_state.current_model = OPENAI_MODEL
        st.session_state.current_base_url = OPENAI_BASE_URL

    # Sidebar para configura√ß√µes
    with st.sidebar:
        st.header("‚öôÔ∏è Configura√ß√µes")
        
        # Configura√ß√µes da API
        st.subheader("ü§ñ API Configuration")
        
        if not st.session_state.api_configured:
            st.warning("‚ö†Ô∏è Configure sua API OpenAI ou compat√≠vel")
            
            # Op√ß√µes de configura√ß√£o
            api_type = st.selectbox(
                "Tipo de API:",
                ["OpenAI Official", "OpenAI Compatible (Local/Custom)"],
                key="api_type_selector"
            )
            
            if api_type == "OpenAI Official":
                openai_key = st.text_input(
                    "OpenAI API Key", 
                    type="password",
                    key="openai_key_input"
                )
                if openai_key:
                    client = create_openai_client(openai_key)
                    if client:
                        st.session_state.openai_client = client
                        st.session_state.current_model = "gpt-3.5-turbo"
                        st.session_state.current_base_url = None
                        st.session_state.api_configured = True
                        st.session_state.api_type = "OpenAI Official"
                        st.success("‚úÖ OpenAI configurado")
                        st.rerun()
            else:
                custom_url = st.text_input(
                    "Base URL:", 
                    placeholder="http://localhost:1234/v1/ ou https://your-api.ngrok.app/v1/",
                    key="custom_url_input"
                )
                custom_model = st.text_input(
                    "Nome do Modelo:", 
                    placeholder="huihui-ai_-_qwen2.5-7b-instruct-abliterated-v2",
                    key="custom_model_input"
                )
                custom_key = st.text_input(
                    "API Key (opcional):", 
                    type="password",
                    help="Deixe vazio se a API local n√£o precisar de key",
                    key="custom_key_input"
                )
                
                if custom_url and custom_model:
                    api_key = custom_key if custom_key else "dummy-key"
                    client = create_openai_client(api_key, custom_url)
                    
                    if client:
                        st.session_state.openai_client = client
                        st.session_state.current_model = custom_model
                        st.session_state.current_base_url = custom_url
                        st.session_state.api_configured = True
                        st.session_state.api_type = "OpenAI Compatible"
                        st.success(f"‚úÖ API Customizada configurada")
                        st.info(f"üîó URL: {custom_url}")
                        st.info(f"ü§ñ Modelo: {custom_model}")
                        st.rerun()
        else:
            # Mostra configura√ß√£o atual
            if st.session_state.api_type == "OpenAI Compatible":
                st.success("‚úÖ API Customizada configurada")
                st.info(f"üîó URL: {st.session_state.current_base_url}")
                st.info(f"ü§ñ Modelo: {st.session_state.current_model}")
            else:
                st.success("‚úÖ OpenAI Oficial configurada")
                st.info(f"ü§ñ Modelo: {st.session_state.current_model}")
            
            # Bot√£o para reconfigurar
            if st.button("üîÑ Reconfigurar API"):
                st.session_state.api_configured = False
                st.session_state.openai_client = None
                st.rerun()
        
        st.markdown("---")
        st.markdown("### üìä Estat√≠sticas")
        
        # Mostrar estat√≠sticas do banco
        conn = sqlite3.connect(DB_NAME)
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM transcripts")
        total_transcripts = cursor.fetchone()[0]
        cursor.execute("SELECT COUNT(DISTINCT topic) FROM transcripts")
        total_topics = cursor.fetchone()[0]
        conn.close()
        
        st.metric("Total de Transcri√ß√µes", total_transcripts)
        st.metric("T√≥picos √önicos", total_topics)
    
    # Tabs para diferentes funcionalidades
    tab1, tab2, tab3 = st.tabs(["üîç Buscar V√≠deos", "üí¨ Chat R√°pido", "üß† Pesquisa Profunda"])
    
    with tab1:
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.header("üîç Buscar V√≠deos")
            
            topic = st.text_input("Digite o assunto:", placeholder="Ex: intelig√™ncia artificial", key="search_topic")
            
            if st.button("üöÄ Buscar e Processar", type="primary"):
                if topic:
                    with st.spinner("Buscando v√≠deos e processando transcri√ß√µes..."):
                        saved_count = rag_system.process_videos(topic)
                    st.rerun()
                else:
                    st.warning("Por favor, digite um assunto.")
        
        with col2:
            # Mostrar v√≠deos processados
            if topic:
                transcripts = rag_system.get_transcripts_by_topic(topic)
                if transcripts:
                    st.subheader(f"üìπ V√≠deos encontrados ({len(transcripts)})")
                    for t in transcripts[:8]:  # Mostra mais v√≠deos
                        st.write(f"‚Ä¢ {t['title'][:60]}...")
    
    with tab2:
        st.header("üí¨ Chat R√°pido com IA")
        
        if "messages" not in st.session_state:
            st.session_state.messages = []
        
        # Input do t√≥pico para chat
        chat_topic = st.text_input("T√≥pico para conversar:", value=topic if 'topic' in locals() else "", key="chat_topic")
        
        # Mostrar hist√≥rico do chat
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        # Input do chat
        if prompt := st.chat_input("Fa√ßa uma pergunta sobre os v√≠deos..."):
            if not chat_topic:
                st.warning("Primeiro digite um t√≥pico!")
            else:
                # Adiciona mensagem do usu√°rio
                st.session_state.messages.append({"role": "user", "content": prompt})
                with st.chat_message("user"):
                    st.markdown(prompt)
                
                # Busca contexto relevante
                with st.chat_message("assistant"):
                    with st.spinner("Pensando..."):
                        context = rag_system.find_relevant_context(prompt, chat_topic)
                        response = rag_system.chat_with_openai(prompt, context)
                    
                    st.markdown(response)
                    
                    # Mostra contexto usado (opcional)
                    with st.expander("üìù Ver contexto usado"):
                        st.text(context[:1000] + "..." if len(context) > 1000 else context)
                
                # Adiciona resposta do assistente
                st.session_state.messages.append({"role": "assistant", "content": response})
    
    with tab3:
        st.header("üß† Pesquisa Profunda")
        st.markdown("*An√°lise sistem√°tica que simula o processo humano de pesquisa*")
        
        deep_research = DeepResearchSystem(rag_system)
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.subheader("üìã Configura√ß√£o da Pesquisa")
            
            research_topic = st.text_input(
                "T√≥pico base:", 
                value=topic if 'topic' in locals() else "",
                placeholder="Ex: intelig√™ncia artificial",
                help="O t√≥pico dos v√≠deos que ser√£o analisados",
                key="research_topic"
            )
            
            research_query = st.text_area(
                "Pergunta de pesquisa:",
                placeholder="Ex: Como a IA est√° impactando o mercado de trabalho? Quais s√£o os principais riscos e benef√≠cios?",
                help="A pergunta espec√≠fica que voc√™ quer responder",
                height=100
            )
            
            output_type = st.selectbox(
                "Tipo de sa√≠da:",
                options=["resumo", "script", "analise", "artigo"],
                format_func=lambda x: {
                    "resumo": "üìÑ Resumo Executivo",
                    "script": "üé• Script de V√≠deo", 
                    "analise": "üìä An√°lise Profunda",
                    "artigo": "üìù Artigo Completo"
                }[x]
            )
            
            # Op√ß√£o de otimiza√ß√£o para modelos limitados
            st.divider()
            use_token_optimization = st.checkbox(
                "üîß Modo Otimizado (32K tokens)",
                value=False,
                help="Use esta op√ß√£o se seu modelo tem limita√ß√£o de 32K tokens. Processa em chunks menores."
            )
            
            if use_token_optimization:
                st.info("üí° **Modo Otimizado ativado**: Ideal para modelos locais com limita√ß√£o de tokens")
        
        with col2:
            st.subheader("‚öôÔ∏è Processo Inteligente")
            st.write("**1.** üîç Busca v√≠deos relacionados")
            st.write("**2.** üß† Analisa cada v√≠deo individualmente")
            st.write("**3.** üî¨ Identifica pontos para aprofundar")
            st.write("**4.** ü§î Pergunta se quer buscar mais conte√∫do")
            st.write("**5.** üìö Busca v√≠deos espec√≠ficos (opcional)")
            st.write("**6.** üîó S√≠ntese final expandida")
            st.write("**7.** ‚ú® Gera conte√∫do profissional")
            
            st.info("üí° **Novo**: O sistema identifica automaticamente lacunas e sugere aprofundamentos!")
            st.success("üöÄ **Resultado**: Pesquisa mais completa e detalhada")
        
        if st.button("üöÄ Iniciar Pesquisa Profunda", type="primary", key="deep_research_btn"):
            if not research_topic:
                st.error("‚ùå Por favor, digite um t√≥pico base")
            elif not research_query:
                st.error("‚ùå Por favor, fa√ßa uma pergunta de pesquisa")
            else:
                st.divider()
                st.header("üî¨ Processo de Pesquisa em Andamento")
                
                # Executa a pesquisa profunda
                with st.spinner("Iniciando pesquisa profunda..."):
                    if use_token_optimization:
                        # Usa pesquisa otimizada para modelos limitados
                        optimized_research = TokenOptimizedResearch(rag_system)
                        result = optimized_research.conduct_optimized_research(
                            topic=research_topic,
                            research_query=research_query,
                            output_type=output_type
                        )
                    else:
                        # Usa pesquisa normal completa
                        result = deep_research.conduct_deep_research(
                            topic=research_topic,
                            research_query=research_query,
                            output_type=output_type
                        )
                
                if result["status"] == "success":
                    # Mostra resultado com informa√ß√µes de otimiza√ß√£o se aplic√°vel
                    if result.get("token_optimized"):
                        st.success(f"‚úÖ Pesquisa otimizada conclu√≠da! Analisados {result['analyses_count']}/{result['total_videos']} v√≠deos")
                        if result.get("estimated_tokens_saved", 0) > 0:
                            st.info(f"üîß Tokens economizados: ~{result['estimated_tokens_saved']:,}")
                    else:
                        st.success(f"‚úÖ Pesquisa conclu√≠da! Analisados {result['analyses_count']}/{result['total_videos']} v√≠deos")
                    
                    st.divider()
                    st.header(f"üìã Resultado Final: {output_type.title()}")
                    
                    # Mostra o resultado final
                    st.markdown(result["final_result"])
                    
                    # Bot√£o para baixar resultado
                    st.download_button(
                        label=f"‚¨áÔ∏è Baixar {output_type.title()}",
                        data=result["final_result"],
                        file_name=f"{output_type}_{research_topic.replace(' ', '_')}.txt",
                        mime="text/plain"
                    )
                    
                    # Mostra estat√≠sticas
                    with st.expander("üìä Estat√≠sticas da Pesquisa"):
                        st.metric("V√≠deos Analisados", result['analyses_count'])
                        st.metric("Total de V√≠deos Encontrados", result['total_videos'])
                        st.metric("Taxa de Sucesso", f"{(result['analyses_count']/result['total_videos']*100):.1f}%")
                        
                        if result.get("token_optimized"):
                            st.metric("Modo", "üîß Otimizado (32K)")
                            if result.get("estimated_tokens_saved", 0) > 0:
                                st.metric("Tokens Economizados", f"~{result['estimated_tokens_saved']:,}")
                        else:
                            st.metric("Modo", "üöÄ Completo")
                
                else:
                    st.error(f"‚ùå {result['message']}")

if __name__ == "__main__":
    main() 